{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9616cbc8-3081-4ed5-a0d3-2cd7c56b5316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/pandeyraj/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3689de33-9c86-48df-9331-9f24e3727cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [nltk]━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [nltk]b]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.1 joblib-1.5.2 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ee5a40-6b6a-4906-aa64-a299b14a62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome, Arpita this side. \n",
    "I am learning about corpus in NLP.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bab5c104-e264-4e13-8e7f-75c8340a252d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Welcome, Arpita this side. \\nI am learning about corpus in NLP.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "760f4764-d908-4563-a4a7-3c46e87e0c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, Arpita this side. \n",
      "I am learning about corpus in NLP.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f55635e-8987-4a0b-8bc2-ea9b2976205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization\n",
    "##sentence-->paragraphs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ceedd1b-8838-4953-af21-3ba4bf70944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57821a68-6cb2-4abf-a5a1-74527427bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b48f902-4c83-402f-9f4c-a51d47bf6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, Arpita this side.\n",
      "I am learning about corpus in NLP.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08192373-2f4c-4890-adff-ebda72bb4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example corpus\n",
    "corpus = \"Natural Language Processing is fun. Tokenization is the first step.\"\n",
    "\n",
    "# Tokenize into words\n",
    "tokens = word_tokenize(corpus)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5161383f-53ff-4109-856c-28f6497cefa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'Arpita', 'this', 'side', '.']\n",
      "['I', 'am', 'learning', 'about', 'corpus', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    " print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bc48b43-458a-4649-a082-962c08de55b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'Arpita',\n",
       " 'this',\n",
       " 'side',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'corpus',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db1c46f8-b9f8-44c3-92a3-e43efb7cafd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'Arpita',\n",
       " 'this',\n",
       " 'side.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'corpus',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0190ffa8-b56d-4f5c-86ff-a42dd91e53da",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '→' (U+2192) (792081846.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m- running → run\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '→' (U+2192)\n"
     ]
    }
   ],
   "source": [
    "<h2 style=\"color:green;\">Stemming</h2>\n",
    "\n",
    "**What Is Stemming?**  \n",
    "Stemming is a technique in Natural Language Processing (NLP) that reduces words to their base or root form (called a stem).\n",
    "\n",
    "<h2 style=\"color:blue;\">Examples</h2>\n",
    "\n",
    "- running → run  \n",
    "- studies → studi  \n",
    "- connected → connect  \n",
    "\n",
    "<h2 style=\"color:blue;\">Why It's Useful</h2>\n",
    "\n",
    "- Improves search engines  \n",
    "- Helps with text classification  \n",
    "- Makes information retrieval more efficient  \n",
    "\n",
    "<h2 style=\"color:blue;\">Stemming vs. Lemmatization</h2>\n",
    "\n",
    "**Stemming:** fast, rule-based, sometimes produces non-words  \n",
    "\n",
    "<h2 style=\"color:green;\">Lemmatization</h2>\n",
    "\n",
    "Uses vocabulary + grammar, produces real words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1547f593-dc02-4bd8-9985-3327fe346aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classification Problem \n",
    "#### Comments of a product is a positive review or negative review \n",
    "#### Reviews-->eating, eat, eaten [going, gone, goes]-->go words=[\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"program\"]\n",
    "words=[\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"program\", \"history\", \"finally\", \"finalized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd69669-4190-4753-8af8-71908a974a31",
   "metadata": {},
   "source": [
    "<h2 style=\"color:teal;\"># PorterStemmer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab4345b-7e13-4c3f-bcf3-1a77b8764b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[32m      2\u001b[39m stemming=PorterStemmer()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwords\u001b[49m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(word+\u001b[33m\"\u001b[39m\u001b[33m---->\u001b[39m\u001b[33m\"\u001b[39m+stemming.stem(word))\n",
      "\u001b[31mNameError\u001b[39m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c34fc-ccd4-49e5-8a4c-9f9f908b4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2 style=\"color:teal;\">#RegexStemmer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048a563-0c94-4ff6-85aa-ec231429ee0b",
   "metadata": {},
   "source": [
    "\n",
    "A **RegexStemmer** is a **custom stemming tool** in Natural Language Processing (NLP) that reduces words to their **root forms** by removing known **prefixes** or **suffixes** using **regular expressions (regex)**.  \n",
    "\n",
    "Unlike rule-based stemmers like **PorterStemmer**, a RegexStemmer allows users to **define their own patterns** for stemming, making it **flexible and customizable**, though typically **simpler and less sophisticated**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76304ac7-1fdf-4ad0-9c23-d32abf26ab2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stmmer=RegexpStemmer('ing|s$|e$|able$',min=4)\n",
    "reg_stmmer.stem('eating')\n",
    "reg_stmmer.stem('ingeating')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f46bb-6707-42dd-8079-15f7d3971db4",
   "metadata": {},
   "source": [
    "<h2 style=\"color:teal;\">#Snowball Stemmer</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8bbd641-3e04-4ad3-ba47-f0ec3584ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "program----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballsstemmer=SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+\"----->\"+snowballsstemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b434ad1-9c88-48b8-91fc-e99c65fa497b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
